{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dc0af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# atleast an educated guess "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f91ca0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.linalg import det\n",
    "\n",
    "from brut_model import BaseModel\n",
    "from dataset_builders import ScaleCellDataset\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "# For reproducibility\n",
    "random_seed = 1869\n",
    "random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e221c801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def print_training_history(history, ck_time, title='loss and accuracy evolution'):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    x = [i * ck_time for i in range(len(history['validation_loss']))]\n",
    "    ax1.plot(x, history['train_loss'], label='training_loss', alpha=0.5, c='g')\n",
    "    ax1.plot(x, history['validation_loss'], label='validation_loss', alpha=0.5, c='r')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.legend()\n",
    "\n",
    "#     ax2.plot(x, history['train_accuracy'], label='training_accuracy', alpha=0.5, c='g')\n",
    "#     ax2.plot(x, history['validation_accuracy'], label='validation_accuracy', alpha=0.5, c='r')\n",
    "#     ax2.set_ylabel('accuracy')\n",
    "#     ax2.set_xlabel('epoch')\n",
    "#     ax2.legend()\n",
    "\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "063b4cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Training_hyper_parameters={\n",
    "    \n",
    "    'batch_size':10,\n",
    "    'validation_split':0.3,\n",
    "    \n",
    "    'learning_rate':0.0001,\n",
    "    'momentum':0.9,\n",
    "    \n",
    "    'nr_epochs': 3000, #8001,\n",
    "    \n",
    "    'checking_epochs': 10 # Save the model performances once at 10 epochs\n",
    "}\n",
    "\n",
    "Model_hyper_parameters={\n",
    "    'inp_shape':8,\n",
    "    'output_shape':4\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425a9f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artificial data \n",
    "nr_elements = 1000\n",
    "xa = []\n",
    "xb = []\n",
    "ta = []\n",
    "for i in range(nr_elements):\n",
    "    ax= [[random.uniform(-1, 20.7),random.uniform(-1, 20.7)],[random.uniform(-1, 20.7),random.uniform(-1, 20.7)]]\n",
    "    \n",
    "    #at = [[random.randint(1, 20),random.randint(1, 10)],[random.randint(1, 10),random.randint(1, 20)]]\n",
    "    at = [[random.randint(1, 20),0],[0,random.randint(1, 20)]]\n",
    "    bt = [[random.randint(1, 20),0],[0,random.randint(1, 20)]]\n",
    "    \n",
    "    bx = np.dot(np.linalg.inv(bt),np.dot(at,ax))\n",
    "    \n",
    "    xa.append(ax)\n",
    "    xb.append(bx)\n",
    "    ta.append(at)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aae197ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[15.869020672853061, 11.922455517212358],\n",
       " [13.1917101660962, 12.156955779850394]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc3e19e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature batch shape: torch.Size([10, 8])\n",
      "Labels batch shape: torch.Size([10, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "dataset = ScaleCellDataset(xa,xb,ta)\n",
    "\n",
    "# Split the data \n",
    "\n",
    "batch_size = Training_hyper_parameters['batch_size']\n",
    "validation_split = Training_hyper_parameters['validation_split']\n",
    "shuffle_dataset = True\n",
    "\n",
    "\n",
    "\n",
    "# Creating data indices for training and validation splits:\n",
    "dataset_size = len(dataset)\n",
    "indices = list(range(dataset_size))\n",
    "split = int(np.floor(validation_split * dataset_size))\n",
    "if shuffle_dataset :\n",
    "    np.random.seed(random_seed)\n",
    "    np.random.shuffle(indices)\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "## Creating data loaders for trianing and vlaidation:\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=len(val_indices),\n",
    "                                                sampler=valid_sampler)\n",
    "\n",
    "all_train =torch.utils.data.DataLoader(dataset, batch_size=len(train_indices), sampler=train_sampler)\n",
    "##  \n",
    "train_features, train_labels = next(iter(train_loader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35605809",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "95de1430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model:\n",
      "BaseModel(\n",
      "  (linear1): Linear(in_features=8, out_features=16, bias=True)\n",
      "  (activation): ReLU()\n",
      "  (linear2): Linear(in_features=16, out_features=32, bias=True)\n",
      "  (linear3): Linear(in_features=32, out_features=32, bias=True)\n",
      "  (linear4): Linear(in_features=32, out_features=16, bias=True)\n",
      "  (linear5): Linear(in_features=16, out_features=4, bias=True)\n",
      ")\n",
      "\n",
      "\n",
      "Model params:\n",
      "Parameter containing:\n",
      "tensor([[-0.0214, -0.1073,  0.3524, -0.1922,  0.1619, -0.2661, -0.2365, -0.3396],\n",
      "        [ 0.3225, -0.1999,  0.1852, -0.1864,  0.2283,  0.2992,  0.2092, -0.1368],\n",
      "        [-0.3020, -0.2880,  0.2059,  0.1502,  0.2800, -0.1038, -0.1114, -0.1685],\n",
      "        [ 0.0876,  0.2108, -0.2732, -0.0450, -0.0768, -0.1650, -0.2842, -0.2666],\n",
      "        [ 0.1274,  0.2854,  0.1319, -0.1059, -0.1761,  0.2790,  0.0261,  0.0069],\n",
      "        [ 0.2155,  0.1315,  0.0074, -0.1551, -0.0418, -0.1107, -0.2800, -0.2539],\n",
      "        [-0.3508,  0.0039, -0.3078, -0.1369,  0.1623, -0.1519,  0.1574, -0.1559],\n",
      "        [ 0.2952,  0.1665, -0.0166,  0.0318, -0.2686,  0.0336,  0.0069, -0.2944],\n",
      "        [ 0.1210,  0.0024, -0.0841, -0.2387,  0.0442,  0.0564,  0.0471, -0.1074],\n",
      "        [-0.0751,  0.2512,  0.3371, -0.1962,  0.1773, -0.2574, -0.2098, -0.1450],\n",
      "        [ 0.0270,  0.1617, -0.1402,  0.0904,  0.2373, -0.0178, -0.2484, -0.3070],\n",
      "        [ 0.2450,  0.3487, -0.0880, -0.0548, -0.2419, -0.1108, -0.1576,  0.1613],\n",
      "        [ 0.2913, -0.1120,  0.0595,  0.2264, -0.3216, -0.0518, -0.2652, -0.0780],\n",
      "        [-0.2683, -0.1666, -0.3315, -0.2389,  0.0246, -0.2872, -0.3481, -0.1890],\n",
      "        [ 0.2119,  0.1891, -0.2682, -0.2168, -0.3396, -0.2540,  0.1205,  0.3229],\n",
      "        [-0.2954, -0.2546, -0.1776, -0.0585, -0.1953,  0.0814,  0.3359,  0.2399]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([ 0.0050,  0.2157,  0.2770, -0.1510,  0.2065, -0.0882,  0.2751, -0.2512,\n",
      "         0.1705,  0.2120, -0.3255, -0.1678,  0.0277, -0.2078,  0.0832, -0.0599],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1984, -0.2420,  0.0134, -0.0556,  0.1420, -0.2356, -0.0862, -0.1619,\n",
      "         -0.1282,  0.1198,  0.2470,  0.0464, -0.0301,  0.0519,  0.2402,  0.0568],\n",
      "        [-0.1141,  0.0419,  0.0150, -0.0354,  0.1440,  0.2048, -0.2494, -0.0772,\n",
      "          0.2279,  0.0571, -0.0250, -0.2327,  0.1761, -0.0912, -0.1735, -0.1756],\n",
      "        [ 0.0090,  0.1169, -0.1929, -0.0894,  0.2134, -0.0428, -0.1003, -0.2279,\n",
      "          0.0598,  0.0517, -0.0255, -0.1710, -0.1054, -0.1060,  0.1071, -0.1960],\n",
      "        [-0.0620, -0.1921,  0.0655,  0.0185,  0.2451, -0.1816,  0.1908, -0.0051,\n",
      "          0.0344,  0.0745, -0.0297,  0.0662,  0.1477, -0.2114,  0.0788,  0.1090],\n",
      "        [ 0.0007, -0.0525,  0.0189,  0.1037, -0.2298,  0.1320,  0.2046,  0.1608,\n",
      "         -0.1400, -0.2147,  0.1435,  0.0774, -0.1301, -0.0562,  0.2344, -0.0815],\n",
      "        [-0.2186,  0.1080,  0.0227,  0.0932,  0.1399, -0.2088, -0.2397, -0.2357,\n",
      "          0.0898, -0.1523,  0.0482, -0.0758,  0.0659, -0.0379,  0.1276, -0.1969],\n",
      "        [ 0.0548,  0.0031,  0.0598,  0.1780,  0.0247,  0.1512,  0.1810, -0.1267,\n",
      "          0.1859,  0.1881,  0.0123, -0.1113, -0.0737,  0.2391, -0.1925,  0.2076],\n",
      "        [ 0.2159, -0.1220, -0.1310, -0.2247,  0.1071, -0.1967,  0.0162,  0.2085,\n",
      "          0.0113, -0.0237, -0.1473, -0.1763,  0.2166, -0.1456,  0.0909,  0.0397],\n",
      "        [-0.2311, -0.0394, -0.1756,  0.0537, -0.0220, -0.0304,  0.0871, -0.0211,\n",
      "         -0.1642, -0.2145,  0.1034, -0.2157, -0.1737,  0.1729, -0.2257, -0.1471],\n",
      "        [-0.1440, -0.1380, -0.0328, -0.2073,  0.0290,  0.1195,  0.1236, -0.1119,\n",
      "          0.0030,  0.0289, -0.1341, -0.0780, -0.0580, -0.0520, -0.1256, -0.2401],\n",
      "        [ 0.1457, -0.1409, -0.2267,  0.2243,  0.0250,  0.0137, -0.2002, -0.1632,\n",
      "         -0.1994, -0.0070,  0.2293, -0.0426, -0.0024,  0.0101,  0.0693,  0.1979],\n",
      "        [ 0.2333,  0.2256, -0.2327, -0.1251,  0.0605,  0.2410,  0.1629, -0.1402,\n",
      "         -0.0158,  0.1458,  0.0377,  0.0373, -0.0678,  0.1879, -0.1060,  0.2496],\n",
      "        [ 0.1686,  0.0128,  0.0690, -0.1744,  0.1192, -0.1816,  0.0083, -0.1613,\n",
      "         -0.1239, -0.0441,  0.1619, -0.0021,  0.2226, -0.2121, -0.1950, -0.0665],\n",
      "        [ 0.0420,  0.2390,  0.1479,  0.2135,  0.2445, -0.2373,  0.1862, -0.0775,\n",
      "         -0.0308,  0.1418,  0.0453,  0.1545,  0.1759, -0.0046, -0.1170,  0.1250],\n",
      "        [-0.2481, -0.2442,  0.0487,  0.2430,  0.2145, -0.2294,  0.1060,  0.0837,\n",
      "         -0.0292,  0.1358,  0.1165, -0.1639,  0.2157,  0.0292, -0.0419, -0.1105],\n",
      "        [-0.0464,  0.1282, -0.0669, -0.1456, -0.1179, -0.1702,  0.0754, -0.1815,\n",
      "         -0.1342,  0.0510,  0.1482,  0.1610,  0.1502,  0.1286, -0.1416, -0.2260],\n",
      "        [-0.1953, -0.1235,  0.2277, -0.0647, -0.0795, -0.1902, -0.1150, -0.0442,\n",
      "         -0.1552,  0.2286, -0.0522, -0.1006, -0.0660, -0.1207, -0.1400,  0.1002],\n",
      "        [-0.0113, -0.1639, -0.0843, -0.1152, -0.0624, -0.0750,  0.2366,  0.0854,\n",
      "         -0.2356,  0.2168, -0.1783, -0.0971,  0.1924, -0.2149,  0.0911,  0.2430],\n",
      "        [ 0.0648,  0.0007, -0.2340,  0.0731,  0.1975, -0.1563,  0.2304, -0.0488,\n",
      "         -0.0544, -0.1034,  0.0535, -0.1437,  0.1123, -0.0957, -0.1151,  0.0302],\n",
      "        [-0.1615, -0.1448,  0.2099,  0.0501, -0.1341, -0.1421,  0.2421, -0.2218,\n",
      "          0.0041, -0.2011,  0.0923,  0.0509, -0.1148, -0.1687, -0.1692, -0.0212],\n",
      "        [-0.0154,  0.2046, -0.0273,  0.0359,  0.0344, -0.1974,  0.1943, -0.0698,\n",
      "         -0.0537, -0.1440,  0.1892, -0.1554,  0.1805, -0.1420,  0.0345,  0.0658],\n",
      "        [-0.0922,  0.1011,  0.0016,  0.2094,  0.0237, -0.0557, -0.1463, -0.0511,\n",
      "         -0.1673, -0.2293,  0.1697,  0.1894,  0.2388, -0.0857,  0.2009,  0.0932],\n",
      "        [ 0.1454, -0.2371, -0.0351, -0.1621, -0.0428,  0.0626, -0.0008,  0.0387,\n",
      "         -0.0493,  0.0603,  0.1774, -0.2437, -0.0583, -0.1181, -0.0673,  0.2163],\n",
      "        [ 0.0924,  0.1554, -0.1607, -0.0876, -0.0706,  0.0586,  0.2247, -0.1576,\n",
      "         -0.1861,  0.0026,  0.0415,  0.2229,  0.2097, -0.0822,  0.0781,  0.1066],\n",
      "        [ 0.0509, -0.1943,  0.1442, -0.0332,  0.0341, -0.2121,  0.2278,  0.1108,\n",
      "          0.1325,  0.1154,  0.1439,  0.0197,  0.1352, -0.0313, -0.0669, -0.1741],\n",
      "        [ 0.2076, -0.1966,  0.1712,  0.0869,  0.1167, -0.1040, -0.2255, -0.0663,\n",
      "         -0.2005,  0.2496,  0.1823,  0.0329, -0.2401,  0.0910,  0.1715,  0.1893],\n",
      "        [ 0.1290,  0.2446, -0.2240, -0.0086, -0.1967,  0.0239,  0.2054,  0.2206,\n",
      "         -0.1822, -0.0828,  0.2255, -0.1742, -0.1843,  0.2364, -0.1584, -0.2330],\n",
      "        [-0.0382,  0.2042, -0.0615,  0.2238,  0.2449, -0.0250, -0.0283, -0.1839,\n",
      "         -0.0601, -0.1581, -0.1217,  0.2369,  0.1958,  0.0835, -0.1694, -0.1211],\n",
      "        [ 0.0393,  0.2469, -0.1628, -0.0979,  0.0068,  0.0521,  0.0651, -0.2084,\n",
      "          0.0436,  0.1293, -0.1517, -0.1596, -0.2194,  0.1729,  0.0574,  0.2099],\n",
      "        [ 0.1966,  0.1832, -0.1128, -0.1205,  0.2388, -0.1892,  0.1756, -0.0419,\n",
      "          0.0403, -0.1776, -0.2210,  0.0036,  0.0956,  0.0514,  0.1026,  0.1696],\n",
      "        [-0.0868,  0.0501, -0.1122, -0.0189,  0.1621, -0.0794,  0.1790, -0.1967,\n",
      "          0.1954,  0.0070, -0.0187,  0.0384, -0.0161, -0.2246, -0.2298,  0.2131],\n",
      "        [-0.0949, -0.1999, -0.1672, -0.0259, -0.1340, -0.2101, -0.1250,  0.0451,\n",
      "          0.0321,  0.1423, -0.1645, -0.0096,  0.0065, -0.1641, -0.2193, -0.1511]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1930, -0.1305, -0.0775,  0.0967, -0.1245, -0.1520,  0.2231,  0.0729,\n",
      "         0.1850,  0.0836,  0.0445,  0.1697, -0.2107, -0.0602, -0.1045,  0.0477,\n",
      "        -0.0316,  0.0069,  0.1508, -0.0687, -0.0958,  0.2216, -0.0787,  0.0599,\n",
      "         0.0573,  0.1742,  0.1354, -0.1502, -0.2259, -0.0563,  0.2476,  0.1664],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0595,  0.0825, -0.0872,  ...,  0.0885, -0.0048, -0.1642],\n",
      "        [ 0.0489, -0.0970,  0.0367,  ...,  0.0955,  0.0813,  0.0025],\n",
      "        [ 0.0905,  0.1103,  0.0310,  ..., -0.1621, -0.0256, -0.0298],\n",
      "        ...,\n",
      "        [ 0.1639,  0.1241, -0.0288,  ..., -0.0071,  0.0630, -0.0804],\n",
      "        [ 0.1470,  0.1163, -0.0280,  ..., -0.1640,  0.1575, -0.0667],\n",
      "        [-0.1003, -0.1751,  0.0749,  ...,  0.0289, -0.0336, -0.0325]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0375,  0.0806, -0.1617,  0.1696,  0.0472, -0.1022, -0.0984, -0.1761,\n",
      "        -0.1517, -0.0275,  0.1106, -0.0208, -0.0107, -0.0709, -0.1056, -0.1536,\n",
      "        -0.0806,  0.1653,  0.0926, -0.0260, -0.1189,  0.1737, -0.1376,  0.0211,\n",
      "         0.1007,  0.0088,  0.1309,  0.0428,  0.0348,  0.1367,  0.1678,  0.0719],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1199,  0.1552, -0.1401, -0.0617, -0.0336,  0.1331,  0.1320,  0.0311,\n",
      "         -0.0493,  0.1291, -0.1106,  0.0468, -0.0817,  0.1355, -0.1520, -0.0998,\n",
      "         -0.1304,  0.0626, -0.0784,  0.0639, -0.0236,  0.1747,  0.0584, -0.0227,\n",
      "         -0.0629,  0.0107, -0.1519,  0.0367, -0.0755,  0.0585, -0.1538,  0.0821],\n",
      "        [-0.0433, -0.0768, -0.1197,  0.1544, -0.1704,  0.1217,  0.1065, -0.0745,\n",
      "         -0.0144,  0.0815, -0.0747,  0.0275,  0.0660, -0.0798, -0.0552,  0.0641,\n",
      "         -0.0926, -0.1459, -0.1425, -0.0212, -0.0229,  0.0017,  0.0268,  0.1165,\n",
      "          0.1527, -0.0785, -0.1179,  0.0557,  0.0064,  0.0874, -0.0232, -0.0005],\n",
      "        [ 0.0597,  0.0910,  0.0191,  0.1217, -0.1407,  0.0700,  0.0575,  0.0016,\n",
      "         -0.0070,  0.0578,  0.0739, -0.0545, -0.1426, -0.1296,  0.1681,  0.0837,\n",
      "         -0.0791,  0.0625, -0.1503,  0.0793,  0.1671, -0.1183, -0.1575,  0.0602,\n",
      "          0.1172, -0.0655, -0.0620,  0.1621, -0.0837,  0.1748,  0.1480, -0.0698],\n",
      "        [ 0.0654,  0.0204,  0.1361,  0.1201, -0.1302,  0.0935,  0.1647,  0.0618,\n",
      "          0.0822,  0.0506, -0.0500,  0.1170, -0.1067,  0.0953, -0.0364, -0.0669,\n",
      "          0.1065,  0.0781, -0.1609,  0.0384,  0.1412, -0.1085, -0.1738, -0.0229,\n",
      "          0.0240,  0.1432, -0.1649,  0.1041, -0.1583, -0.0652,  0.1359,  0.0580],\n",
      "        [ 0.1730, -0.1232, -0.0813,  0.1142,  0.0517, -0.0343,  0.1492, -0.1674,\n",
      "         -0.0964, -0.1062, -0.0046,  0.0105, -0.0968,  0.1310,  0.0126, -0.1160,\n",
      "          0.1186,  0.0629,  0.0516, -0.1339, -0.1251,  0.0723,  0.0467,  0.1718,\n",
      "          0.1611,  0.1224,  0.0200,  0.1499,  0.0882,  0.0358,  0.1003, -0.0852],\n",
      "        [ 0.1126, -0.1755,  0.0571,  0.0607, -0.0070, -0.0583, -0.1760, -0.1602,\n",
      "         -0.0422, -0.0263, -0.0736,  0.0949, -0.1335, -0.1071,  0.0709,  0.1238,\n",
      "         -0.0274, -0.1230, -0.1466,  0.1063, -0.0379, -0.1689, -0.1721,  0.0188,\n",
      "         -0.1686,  0.1422,  0.1324, -0.1057, -0.0061,  0.0797,  0.0245,  0.0563],\n",
      "        [-0.1590,  0.0428, -0.0135, -0.0704,  0.0110, -0.1348,  0.0882, -0.0114,\n",
      "         -0.1554, -0.0005, -0.0643,  0.1623, -0.0966,  0.0195,  0.1443,  0.0979,\n",
      "         -0.1091, -0.1522,  0.0404,  0.0834,  0.0333,  0.1327,  0.1477, -0.0240,\n",
      "         -0.0841,  0.0158, -0.1018,  0.0570,  0.1336,  0.0396, -0.0705,  0.1123],\n",
      "        [-0.1106,  0.1501,  0.1297, -0.0675,  0.0606, -0.0194, -0.1613,  0.0910,\n",
      "         -0.0030,  0.0189,  0.1144, -0.1256, -0.0447, -0.1095, -0.1572,  0.0957,\n",
      "         -0.0311, -0.0142, -0.0999,  0.1256,  0.0781,  0.1440,  0.1744, -0.1478,\n",
      "         -0.0078,  0.1180, -0.0818,  0.1563,  0.0664,  0.0560,  0.0793, -0.1559],\n",
      "        [ 0.1615,  0.0975, -0.0181,  0.0765,  0.1018, -0.0635, -0.1433,  0.1040,\n",
      "         -0.0090, -0.0788, -0.0045,  0.1188, -0.0377, -0.0340,  0.0389,  0.0793,\n",
      "         -0.0021, -0.1555, -0.0704, -0.1525,  0.1280,  0.1565,  0.0840,  0.0191,\n",
      "         -0.1589,  0.0153, -0.0243, -0.0325,  0.1767,  0.1681,  0.0146, -0.1360],\n",
      "        [-0.1594, -0.1108,  0.1423, -0.0684,  0.0805, -0.1129, -0.1130, -0.0238,\n",
      "          0.1058, -0.0865,  0.0457,  0.0284, -0.1667,  0.0876, -0.0307, -0.0298,\n",
      "          0.1541,  0.0803,  0.1507, -0.0831, -0.1447,  0.1173,  0.0356, -0.1438,\n",
      "          0.1223, -0.0596, -0.1457, -0.0842, -0.1535,  0.1549,  0.0226, -0.0285],\n",
      "        [ 0.0418, -0.0736,  0.1436, -0.0649, -0.0356,  0.1450, -0.0042, -0.0584,\n",
      "         -0.0382,  0.1734, -0.0040,  0.0150, -0.1609,  0.0560, -0.1511,  0.1415,\n",
      "          0.0534,  0.1543, -0.1035, -0.1347, -0.0115,  0.1760,  0.0111, -0.0584,\n",
      "         -0.1704,  0.1674, -0.1747, -0.1646,  0.0786, -0.0752, -0.0588,  0.0826],\n",
      "        [-0.0650,  0.0361, -0.0632,  0.1288,  0.0935, -0.0517, -0.1507, -0.1226,\n",
      "         -0.1062, -0.0063, -0.0846, -0.0821, -0.1761,  0.1004, -0.0380,  0.0500,\n",
      "          0.1545,  0.0577, -0.0618, -0.1410,  0.0290,  0.1123,  0.1191, -0.1412,\n",
      "          0.0045,  0.1321,  0.0927, -0.0843,  0.0362,  0.0423,  0.0968,  0.1759],\n",
      "        [ 0.1508,  0.0481, -0.0495, -0.0524, -0.0047,  0.1713,  0.1184,  0.0693,\n",
      "          0.1354,  0.0400, -0.1710,  0.1312, -0.0870, -0.0032, -0.1597, -0.1262,\n",
      "          0.0300, -0.0705, -0.1470, -0.0783, -0.1327, -0.0136,  0.0091, -0.0815,\n",
      "         -0.0714,  0.0112, -0.0243,  0.0692,  0.1494,  0.0583, -0.0897, -0.0544],\n",
      "        [-0.0510, -0.1655,  0.1085,  0.1236,  0.0563, -0.0711,  0.1415, -0.0926,\n",
      "          0.0780, -0.0751, -0.1704,  0.0884,  0.0758, -0.1215,  0.0405, -0.0729,\n",
      "         -0.1086, -0.0802, -0.1758, -0.0409,  0.1276, -0.1068,  0.1037,  0.1009,\n",
      "          0.0507,  0.0703,  0.0989, -0.0932, -0.0041,  0.0147, -0.0742, -0.1036],\n",
      "        [-0.0267, -0.0170, -0.1534, -0.1684,  0.1038, -0.1137, -0.0459,  0.1629,\n",
      "         -0.0724, -0.0267,  0.0121,  0.0455, -0.0157,  0.0759, -0.1511,  0.0303,\n",
      "         -0.0118,  0.0780, -0.0225,  0.1122, -0.0145, -0.0586, -0.0480, -0.0151,\n",
      "         -0.1015,  0.1657,  0.1087,  0.0261, -0.0801,  0.1568,  0.0884, -0.0348],\n",
      "        [-0.1159, -0.0399, -0.1105, -0.0987,  0.0578, -0.1668, -0.1343, -0.0476,\n",
      "         -0.1075, -0.0867, -0.1373, -0.0194, -0.0052, -0.1356,  0.1568,  0.0514,\n",
      "          0.0760, -0.0530,  0.0380, -0.1648,  0.0522, -0.0464, -0.0152,  0.0355,\n",
      "          0.0151,  0.0465, -0.0054,  0.0299, -0.1132, -0.1540, -0.0934, -0.0366]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.0310,  0.1348,  0.0658, -0.0788, -0.1415, -0.0712, -0.1105,  0.0987,\n",
      "         0.0715, -0.1219, -0.1186,  0.0179, -0.0942,  0.0047,  0.0549, -0.1717],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.1642,  0.0292,  0.1608, -0.0648, -0.1011, -0.0582,  0.0879,  0.1252,\n",
      "          0.0786,  0.2361,  0.1404,  0.1224, -0.0872,  0.0089,  0.1142, -0.1958],\n",
      "        [ 0.1162,  0.0043, -0.0314, -0.0517,  0.1439,  0.2308, -0.0364,  0.2351,\n",
      "          0.1532, -0.0931, -0.0411,  0.0527, -0.0372, -0.1837, -0.2132, -0.0221],\n",
      "        [-0.1640,  0.0064,  0.0903,  0.0635,  0.0559,  0.1697,  0.2123, -0.0775,\n",
      "          0.0349, -0.0768,  0.2453,  0.0835, -0.1619,  0.0735, -0.1235, -0.2400],\n",
      "        [-0.2481, -0.0753,  0.1417,  0.1865, -0.1713,  0.0125,  0.2439,  0.0070,\n",
      "         -0.1614, -0.1726,  0.1337,  0.0476, -0.1366, -0.0265,  0.0371, -0.0074]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2433, -0.0332,  0.1193,  0.0797], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Prepare a model \n",
    "\n",
    "model = BaseModel(Model_hyper_parameters) # The model that we wish to train.\n",
    "\n",
    "print('The model:')\n",
    "print(model)\n",
    "\n",
    "print('\\n\\nModel params:')\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5235edfb",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a65680d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fbe29b2b700>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def at_sin(x, up =99999999):\n",
    "    \n",
    "    tr_x= (x + 0.5)\n",
    "    \n",
    "    if x<1:\n",
    "        res = up*(1/(1+torch.sqrt((x**2))) )* torch.sin(tr_x*np.pi) +1/(x+0.00000000000001)\n",
    "    else:\n",
    "        res = (x-1)**2 -up*(1/(1+x**2))+1/(x+0.00000000000001)\n",
    "    return res\n",
    "\n",
    "X = torch.tensor([i*0.1 for i in range(0,50,1)])\n",
    "Y= [at_sin(x) for x in X]\n",
    "\n",
    "plt.scatter(X[5:],Y[5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9982787a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tAtB(params,a,b):\n",
    "    tA = torch.tensor([[params[0],params[1]],\n",
    "                   [params[2],params[3]]])\n",
    "    \n",
    "    tB = torch.mm(torch.mm(tA,a), torch.inverse(b)) # tAa=tBb\n",
    "    \n",
    "    return tA, tB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "42083c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_fucntion(params ,a,b, k_p=999,strain_boundery =[[-0.2e-9,0.2e-9],[-0.2e-9,0.2e-9]]):\n",
    "          \n",
    "        tA, tB= tAtB(params,a,b)\n",
    "        tAa= torch.mm(tA,a)\n",
    "        tBb= torch.mm(tB,b)\n",
    "        \n",
    "        \n",
    "#         # Strain tunning \n",
    "#         strain = streined_proces(tB,strain_boundery)\n",
    "#         tB= np.dot(strain,tB)\n",
    "        \n",
    "        \n",
    "#         # main condition\n",
    "#         zero_mat= tAa - tBb\n",
    "#         s=0\n",
    "#         for row in zero_mat:\n",
    "#             for e in row:\n",
    "#                 s+=e*e\n",
    "        \n",
    "        # mimimum TA\n",
    "        detTAa= det(tAa)*det(tAa) # minimum but biger than 0\n",
    "        detTBb= det(tBb)*det(tBb)\n",
    "        \n",
    "        # TB integer \n",
    "        cons =99999\n",
    "        tB_con = 0  \n",
    "        for row in tB:\n",
    "            for e in row:\n",
    "                tB_con += ((torch.round(e)-e))*((torch.round(e)-e)) # e*e\n",
    "        tB_con = tB_con*cons\n",
    "        \n",
    "        tA_lenghth =((tA[0][0]**2+tA[0][1]**2)+(tA[1][0]**2+tA[1][1]**2) )*100\n",
    "        \n",
    "        \n",
    "        f =  at_sin(detTAa)+ tB_con**2 +tA_lenghth\n",
    "        f= torch.autograd.Variable(f, requires_grad=True)\n",
    "        \n",
    "        \n",
    "        return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cfc8ac30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loss function\n",
    "def loss(output, x):\n",
    "#     print(\"o\",output)\n",
    "#     print(\"t\",target)\n",
    "    cost =0 \n",
    "#     print(\"x\",x)\n",
    "#     print(\"output\", output)\n",
    "    for i in range(len(output)):\n",
    "#         print(\"xi\",x[i])\n",
    "        k= output[i]\n",
    "        a= torch.tensor([[x[i][0],x[i][1] ], [x[i][2],x[i][3] ]])\n",
    "        b= torch.tensor([[x[i][4],x[i][5] ], [x[i][6],x[i][7] ]])\n",
    "        c = fit_fucntion(k,a,b)\n",
    "        cost+=c\n",
    "#     cost = torch.sum(a)\n",
    "    #print(\"c\",cost)\n",
    "    return cost\n",
    "\n",
    "criterion = loss # nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "learning_rate = Training_hyper_parameters['learning_rate']\n",
    "momentum = Training_hyper_parameters['momentum']                             \n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d045f103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1| loss:262797757775872.0\n",
      "Epoch 101| loss:262797824884736.0\n",
      "Epoch 201| loss:262797791330304.0\n",
      "Epoch 301| loss:262797858439168.0\n",
      "Epoch 401| loss:262797824884736.0\n",
      "Epoch 501| loss:262797824884736.0\n",
      "Epoch 601| loss:262797808107520.0\n",
      "Epoch 701| loss:262797791330304.0\n"
     ]
    }
   ],
   "source": [
    "# actual training \n",
    "\n",
    "## Work in progress ##\n",
    "\n",
    "nr_epochs = Training_hyper_parameters['nr_epochs']\n",
    "ck_time = Training_hyper_parameters['checking_epochs']\n",
    "\n",
    "history = {'train_loss':[],'validation_loss':[], 'train_accuracy':[] , 'validation_accuracy':[]}\n",
    "\n",
    "for epoch in range(nr_epochs):  \n",
    "    model.train(True)\n",
    "    train_loss = 0.0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        \n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs) # inputs instead of labels \n",
    "        train_loss += loss/len(train_loader)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        print('Epoch {}| loss:{}'.format(epoch + 1,train_loss ))\n",
    "    \n",
    "    if epoch % ck_time == 0:\n",
    "        \n",
    "        #training\n",
    "        for i, data in enumerate(all_train):\n",
    "            inputs_t, labels_t = data\n",
    "            outputs_t = model(inputs_t)\n",
    "            \n",
    "            tl = criterion(outputs_t, inputs_t).detach().numpy()\n",
    "            \n",
    "        \n",
    "        history['train_loss'].append(tl)\n",
    "       \n",
    "        #validation\n",
    "        for i, data in enumerate(validation_loader):\n",
    "            inputs_v, labels_v = data\n",
    "            outputs_v = model(inputs_v)\n",
    "#         print(\"aici\")\n",
    "        vl = criterion(outputs_v, inputs_v).detach().numpy()\n",
    "        \n",
    "        history['validation_loss'].append(vl)\n",
    "        \n",
    "        \n",
    "        \n",
    "print('-Training finished-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df385e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82131ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training evolution:\n",
    "\n",
    "fig_trainig_evolution = print_training_history(history, ck_time)\n",
    "\n",
    "# print(\"\\nModel performance:\")\n",
    "# for k in history:\n",
    "#     print(\"{}\".format(k),history[k][len(history[k])-1])\n",
    "# print(\"__________________\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac0428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def print_training_history(history, ck_time, title='loss and accuracy evolution', cut=400):\n",
    "    fig, (ax1, ax2) = plt.subplots(2, 1)\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    x = [i * ck_time for i in range(len(history['validation_loss'][100:cut]))]\n",
    "    ax1.plot(x, history['train_loss'][100:cut], label='training_loss', alpha=0.5, c='g')\n",
    "    ax1.plot(x, history['validation_loss'][100:cut], label='validation_loss', alpha=0.5, c='r')\n",
    "    ax1.set_ylabel('loss')\n",
    "    ax1.set_xlabel('epoch')\n",
    "    ax1.legend()\n",
    "    \n",
    "fig_trainig_evolution = print_training_history(history, ck_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([[0,1],[1,0]])\n",
    "b = np.array([[0,5],[5,0]])\n",
    "xa_v= torch.tensor([0.0,1.0,1.0,0.0,0.0,5.0,5.0,0.0])\n",
    "outputs_v = model(xa_v)\n",
    "print(torch.reshape(outputs_v,(2,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f312572",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
